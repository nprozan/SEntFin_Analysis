{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qm3kQEi9iy6"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl83C6JV4dsz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8fc8c3e-b4e6-4d63-8961-573622c784f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers\n",
        "!pip install -U tensorflow==2.11 --quiet\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "\n",
        "import sklearn as sk\n",
        "import os\n",
        "import nltk\n",
        "from nltk.data import find\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "import ast\n",
        "from keras import backend as K\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RopIp5p_DV2"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizerFast, TFRobertaModel, TFRobertaModel, TFAutoModelForSequenceClassification, AutoTokenizer\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "model = TFRobertaModel.from_pretrained('roberta-base',output_hidden_states=True, output_attentions=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer2 = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "\n",
        "model2 = TFAutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\", output_hidden_states=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2GnRbWuvXZC",
        "outputId": "b7237d52-81ba-41ae-88a3-8c0853d66d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at ProsusAI/finbert and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')\n",
        "sentfin = pd.read_csv('/content/gdrive/MyDrive/Raw Data/SEntFiN-v1.1.csv')"
      ],
      "metadata": {
        "id": "9YbYmspu-SwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3XyXd4F4nUj"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input, Flatten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pREWJfPC4wGH"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "sentfin = pd.read_csv('/content/gdrive/MyDrive/Raw Data/SEntFiN-v1.1.csv')\n",
        "\n",
        "\n",
        "X = np.arange(0,10753)\n",
        "y = np.arange(0,10753)\n",
        "x_train, x_test, y_train, y_test = train_test_split(sentfin['Title'], sentfin['Decisions'], test_size=0.3,  random_state=8)\n",
        "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.6660, random_state=8)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_val), len(y_val), len(x_test), len(y_test), len(y_train), len(x_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVA5XPV_YVZh",
        "outputId": "d9a9ba7a-bfb2-48e4-f5bc-1ff921b05a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2149, 2149, 1077, 1077, 7527, 7527)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EksqlkocFZQ8"
      },
      "outputs": [],
      "source": [
        "x_train[10680] = 'Global oil worries, rupee fall weigh on stocks'\n",
        "\n",
        "\n",
        "\n",
        "y_train[220] = '{\"Nifty50\": \"neutral\"}'\n",
        "y_train[792] =  '{\"Alibaba\": \"neutral\"}'\n",
        "y_train[6056] =  '{\"IIHFL\": \"neutral\"}'\n",
        "y_train[10701] = '{\"Australia\": \"negative\", \"NZ dlrs\": \"negative\"}'\n",
        "y_train[9451] = '{\"Gold\": \"negative\"}'\n",
        "y_train[9181] = '{\"Brent\": \"negative\"}'\n",
        "\n",
        "\n",
        "y_train[10166] = '{\"Bharti\": \"positive\", \"Tata Comm\": \"positive\", \"mobile content players\": \"positive\"}'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sR7B7BfiB-XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb3dswJ35Tr_"
      },
      "source": [
        "## Tokenizing the Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WL8pWnc_8BY"
      },
      "outputs": [],
      "source": [
        "### Function Helps catch all special cases in the tokenization function\n",
        "\n",
        "def return_helper(ex_title, ex):\n",
        "    punctuations = [':', ',', \"'s\", ';', '?', \"'\", 's', \"s'\"]\n",
        "\n",
        "    for punctuation in punctuations:\n",
        "        pattern = re.escape(ex + punctuation)\n",
        "        match = re.search(pattern, ex_title)\n",
        "        if match:\n",
        "            return match.start()\n",
        "\n",
        "    return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEonnSEMoxhu"
      },
      "outputs": [],
      "source": [
        "def clean_data2(x_data,y_data, model = model, tokenizer = tokenizer):\n",
        "  '''input: news headline data set with sentiments\n",
        "  output: roberta tokenized inputs, and corresponding label\n",
        "  '''\n",
        "\n",
        "  title =[str(i) for i in x_data.values]\n",
        "  headlines = []\n",
        "  delete = []\n",
        "\n",
        "  for row in title:\n",
        "    headline = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in row]).split())\n",
        "    headlines.append(headline)\n",
        "  try:\n",
        "    decisions = ([json.loads(i) for i in y_data])\n",
        "  except:\n",
        "    decisions = (([ast.literal_eval(i) for i in y_data]))\n",
        "  entities = []\n",
        "  sents = []\n",
        "  for ex in decisions:\n",
        "    r_entities = []\n",
        "    r_sents = []\n",
        "    for key in ex.keys():\n",
        "      r_entities.append(\" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in key]).split()))\n",
        "    for value in ex.values():\n",
        "      r_sents.append(value)\n",
        "    entities.append(r_entities)\n",
        "    sents.append(r_sents)\n",
        "\n",
        "  title_tokenizer = tokenizer(headlines, padding='max_length', max_length=30, truncation=True, return_tensors=\"tf\")\n",
        "  text_tokenized = tokenizer.batch_encode_plus(headlines, padding='max_length', max_length=30, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "  problems = 0\n",
        "  i = 0\n",
        "  fin_labels = []\n",
        "  while i < len(y_data):\n",
        "\n",
        "\n",
        "    ex_title = headlines[i].split()\n",
        "\n",
        "\n",
        "    indexes = []\n",
        "\n",
        "    input = np.array(text_tokenized.word_ids(i))\n",
        "\n",
        "    for ex_entity in entities[i]:\n",
        "      word_ent = []\n",
        "      ex = ex_entity.split()[0]\n",
        "      try:\n",
        "        return_index = ex_title.index(ex)\n",
        "\n",
        "        matches = [j for j,val in enumerate(input) if val==return_index]\n",
        "\n",
        "        for matching in matches:\n",
        "          word_ent.append(matching)\n",
        "\n",
        "      except:\n",
        "        helper = return_helper(ex_title, ex)\n",
        "        if helper is None:\n",
        "          problems = problems + 1\n",
        "\n",
        "          delete.append(i)\n",
        "          continue\n",
        "        return_index = helper\n",
        "        matches = [j for j,val in enumerate(input) if val==return_index]\n",
        "        for matching in matches:\n",
        "          word_ent.append(matching)\n",
        "      if len(ex_entity.split()) > 1:\n",
        "        for ex in ex_entity.split()[1:]:\n",
        "\n",
        "          return_index = return_index + 1\n",
        "          matches = [j for j,val in enumerate(input) if val==return_index]\n",
        "          for matching in matches:\n",
        "              word_ent.append(matching)\n",
        "\n",
        "\n",
        "\n",
        "      indexes.append(word_ent)\n",
        "\n",
        "    return_label = np.zeros(30)\n",
        "    z = 0\n",
        "    while z < len(indexes):\n",
        "      x = 0\n",
        "      while x < len(indexes[z]):\n",
        "        if x == 0:\n",
        "\n",
        "          if sents[i][z] == 'positive':\n",
        "            return_label[indexes[z][x]] = 4\n",
        "          elif sents[i][z] == 'neutral':\n",
        "            return_label[indexes[z][x]] = 3\n",
        "          else:\n",
        "            return_label[indexes[z][x]] = 2\n",
        "        else:\n",
        "          return_label[indexes[z][x]] = 1\n",
        "        x = x + 1\n",
        "      z = z + 1\n",
        "\n",
        "\n",
        "\n",
        "    fin_labels.append(return_label)\n",
        "    i = i+1\n",
        "\n",
        "\n",
        "  labels = tf.convert_to_tensor(fin_labels, dtype=tf.float32)\n",
        "\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((\n",
        "      dict(title_tokenizer),\n",
        "      labels\n",
        "  ))\n",
        "\n",
        "  return dataset, dict(title_tokenizer), labels, delete,  title , headlines, decisions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tf8vP8_C5MaP"
      },
      "outputs": [],
      "source": [
        "train_dataset, train_tokens, trained_labels,train_delete, train_sentence, train_headline, train_decision = clean_data2(x_train,y_train,model, tokenizer)\n",
        "\n",
        "\n",
        "test_dataset, test_tokens, test_labels, test_delete, test_sentence, test_headline, test_decision= clean_data2(x_test,y_test,model, tokenizer)\n",
        "val_dataset, val_tokens, valid_labels,val_delete, val_sentence, val_headline, val_decision  = clean_data2(x_val, y_val,model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MKE1Nj1YYz2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L28CQUoN7xof"
      },
      "outputs": [],
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "train_labels = to_categorical(trained_labels, num_classes=5)\n",
        "train_labels = tf.convert_to_tensor(train_labels )\n",
        "\n",
        "val_labels = to_categorical(valid_labels, num_classes=5)\n",
        "val_labels = tf.convert_to_tensor(val_labels )\n",
        "\n",
        "test_labels = to_categorical(test_labels, num_classes=5)\n",
        "test_labels = tf.convert_to_tensor(test_labels )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLCrU4RB71hW"
      },
      "source": [
        "## Roberta Model (used for standard and w/ pretrain function)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bsf9G0q6d44"
      },
      "outputs": [],
      "source": [
        "max_length = 30\n",
        "def create_roberta_multiclass_model(model = model,\n",
        "                                 num_classes = 5,\n",
        "                                 hidden_size = 30,\n",
        "                                 dropout=0.2,\n",
        "                                 learning_rate=0.00005):\n",
        "\n",
        "    keras.backend.clear_session()\n",
        "    bert_model = model\n",
        "\n",
        "\n",
        "    bert_model.trainable = True\n",
        "\n",
        "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='input_ids')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='attention_mask')\n",
        "\n",
        "    bert_inputs = {'input_ids': input_ids,\n",
        "                  'attention_mask': attention_mask}\n",
        "\n",
        "    bert_out = bert_model(bert_inputs)\n",
        "\n",
        "    output = bert_out['last_hidden_state']\n",
        "\n",
        "\n",
        "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer_2')(output)\n",
        "\n",
        "    hidden = tf.keras.layers.Dropout(dropout)(hidden)\n",
        "\n",
        "    hidden2 = tf.keras.layers.Dense(hidden_size/3, activation='relu', name='hidden_layer_3')(hidden)\n",
        "\n",
        "    hidden2 = tf.keras.layers.Dropout(dropout)(hidden2)\n",
        "\n",
        "    classification = tf.keras.layers.Dense(num_classes, activation='softmax',name='classification_layer')(hidden2)\n",
        "\n",
        "    classification_model = tf.keras.Model(inputs=[input_ids,attention_mask], outputs=[classification])\n",
        "\n",
        "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "                                metrics=['accuracy', tf.keras.metrics.Precision(thresholds=0, class_id = 2), ])\n",
        "\n",
        "    return classification_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnlDvEUV5uor"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "roberta_model = create_roberta_multiclass_model( num_classes=5, hidden_size=120, dropout = 0.15)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNbBqH6-7iZW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3269b2e-81c9-4886-922e-9704a4bde5ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " attention_mask (InputLayer)    [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " input_ids (InputLayer)         [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['attention_mask[0][0]',         \n",
            " el)                            thPoolingAndCrossAt               'input_ids[0][0]']              \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 30,                                                \n",
            "                                768),                                                             \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=(                                               \n",
            "                                (None, 30, 768),                                                  \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768)),                                                \n",
            "                                 attentions=((None,                                               \n",
            "                                 12, None, 30),                                                   \n",
            "                                 (None, 12, None, 3                                               \n",
            "                                0),                                                               \n",
            "                                 (None, 12, None, 3                                               \n",
            "                                0),                                                               \n",
            "                                 (None, 12, None, 3                                               \n",
            "                                0),                                                               \n",
            "                                 (None, 12, None, 3                                               \n",
            "                                0),                                                               \n",
            "                                 (None, 12, None, 3                                               \n",
            "                                0),                                                               \n",
            "                                 (None, 12, None, 3                                               \n",
            "                                0),                                                               \n",
            "                                 (None, 12, None, 3                                               \n",
            "                                0),                                                               \n",
            "                                 (None, 12, None, 3                                               \n",
            "                                0),                                                               \n",
            "                                 (None, 12, None, 3                                               \n",
            "                                0),                                                               \n",
            "                                 (None, 12, None, 3                                               \n",
            "                                0),                                                               \n",
            "                                 (None, 12, None, 3                                               \n",
            "                                0)),                                                              \n",
            "                                 cross_attentions=N                                               \n",
            "                                one)                                                              \n",
            "                                                                                                  \n",
            " hidden_layer_2 (Dense)         (None, 30, 120)      92280       ['tf_roberta_model[0][25]']      \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 30, 120)      0           ['hidden_layer_2[0][0]']         \n",
            "                                                                                                  \n",
            " hidden_layer_3 (Dense)         (None, 30, 40)       4840        ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 30, 40)       0           ['hidden_layer_3[0][0]']         \n",
            "                                                                                                  \n",
            " classification_layer (Dense)   (None, 30, 5)        205         ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 124,742,957\n",
            "Trainable params: 124,742,957\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "roberta_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J599meMn8ciX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a01caa8a-4fd7-46b8-f389-d63f11acd8c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "471/471 [==============================] - 143s 193ms/step - loss: 0.0674 - accuracy: 0.9417 - precision: 0.0119 - val_loss: 0.0341 - val_accuracy: 0.9656 - val_precision: 0.0114\n",
            "Epoch 2/6\n",
            "471/471 [==============================] - 78s 166ms/step - loss: 0.0343 - accuracy: 0.9701 - precision: 0.0119 - val_loss: 0.0255 - val_accuracy: 0.9791 - val_precision: 0.0114\n",
            "Epoch 3/6\n",
            "471/471 [==============================] - 76s 161ms/step - loss: 0.0248 - accuracy: 0.9810 - precision: 0.0119 - val_loss: 0.0233 - val_accuracy: 0.9795 - val_precision: 0.0114\n",
            "Epoch 4/6\n",
            "471/471 [==============================] - 76s 161ms/step - loss: 0.0203 - accuracy: 0.9843 - precision: 0.0119 - val_loss: 0.0331 - val_accuracy: 0.9769 - val_precision: 0.0114\n",
            "Epoch 5/6\n",
            "471/471 [==============================] - 76s 161ms/step - loss: 0.0217 - accuracy: 0.9836 - precision: 0.0119 - val_loss: 0.0296 - val_accuracy: 0.9799 - val_precision: 0.0114\n",
            "Epoch 6/6\n",
            "471/471 [==============================] - 75s 160ms/step - loss: 0.0166 - accuracy: 0.9867 - precision: 0.0119 - val_loss: 0.0263 - val_accuracy: 0.9803 - val_precision: 0.0114\n"
          ]
        }
      ],
      "source": [
        "\n",
        "roberta_model_history = roberta_model.fit(train_tokens, train_labels,\n",
        "\n",
        "                                                  validation_data=(val_tokens, val_labels),\n",
        "                                                  batch_size=16,\n",
        "                                                  epochs=6\n",
        "                                                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##FINBERT\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ZsAIYXjD_KV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 30\n",
        "def create_finbert_multiclass_model(model = model,\n",
        "                                 num_classes = 5,\n",
        "                                 hidden_size = 30,\n",
        "                                 dropout=0.2,\n",
        "                                 learning_rate=0.00005):\n",
        "\n",
        "    keras.backend.clear_session()\n",
        "    bert_model = model\n",
        "\n",
        "\n",
        "    bert_model.trainable = True\n",
        "\n",
        "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='input_ids')\n",
        "\n",
        "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='attention_mask')\n",
        "    #token_type_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='token_type_ids_layer')\n",
        "\n",
        "    bert_inputs = {'input_ids': input_ids,\n",
        "                  'attention_mask': attention_mask}\n",
        "                  # 'token_type_ids_layer': token_type_ids}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    bert_out = bert_model(bert_inputs)\n",
        "\n",
        "    output = bert_out['hidden_states'][12]\n",
        "\n",
        "\n",
        "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer_2')(output)\n",
        "\n",
        "    hidden = tf.keras.layers.Dropout(dropout)(hidden)\n",
        "\n",
        "    hidden2 = tf.keras.layers.Dense(hidden_size/3, activation='relu', name='hidden_layer_3')(hidden)\n",
        "\n",
        "    hidden2 = tf.keras.layers.Dropout(dropout)(hidden2)\n",
        "\n",
        "    classification = tf.keras.layers.Dense(num_classes, activation='softmax',name='classification_layer')(hidden2)\n",
        "\n",
        "    classification_model = tf.keras.Model(inputs=[input_ids,attention_mask], outputs=[classification])\n",
        "\n",
        "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "                                metrics=['accuracy', tf.keras.metrics.Precision(thresholds=0, class_id = 2), ])\n",
        "\n",
        "    return classification_model"
      ],
      "metadata": {
        "id": "3iNrFaMHweYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finbert_model = create_finbert_multiclass_model( num_classes=5, hidden_size=120, dropout = 0.2,model = model2)\n",
        "finbert_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4XzWe3fwkl6",
        "outputId": "8cf6d1d4-00ea-4718-ec2d-d6d032c7be0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " attention_mask (InputLayer)    [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " input_ids (InputLayer)         [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_for_sequence_classific  TFSequenceClassifie  109484547  ['attention_mask[0][0]',         \n",
            " ation (TFBertForSequenceClassi  rOutput(loss=None,               'input_ids[0][0]']              \n",
            " fication)                      logits=(None, 3),                                                 \n",
            "                                 hidden_states=((No                                               \n",
            "                                ne, 30, 768),                                                     \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768),                                                 \n",
            "                                 (None, 30, 768)),                                                \n",
            "                                 attentions=None)                                                 \n",
            "                                                                                                  \n",
            " hidden_layer_2 (Dense)         (None, 30, 120)      92280       ['tf_bert_for_sequence_classifica\n",
            "                                                                 tion[0][12]']                    \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 30, 120)      0           ['hidden_layer_2[0][0]']         \n",
            "                                                                                                  \n",
            " hidden_layer_3 (Dense)         (None, 30, 40)       4840        ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 30, 40)       0           ['hidden_layer_3[0][0]']         \n",
            "                                                                                                  \n",
            " classification_layer (Dense)   (None, 30, 5)        205         ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,581,872\n",
            "Trainable params: 109,581,872\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finbert_history = finbert_model.fit(train_tokens, train_labels,\n",
        "\n",
        "                                                  validation_data=(val_tokens, val_labels),\n",
        "                                                  batch_size=16,\n",
        "                                                  epochs=5\n",
        "                                                  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlEBLg49zMSb",
        "outputId": "95edb2af-1408-4f30-ac58-ef8d08fd3176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['token_type_ids'] which did not match any model input. They will be ignored by the model.\n",
            "  inputs = self._flatten_to_reference_inputs(inputs)\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0', 'tf_bert_for_sequence_classification/classifier/kernel:0', 'tf_bert_for_sequence_classification/classifier/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0', 'tf_bert_for_sequence_classification/classifier/kernel:0', 'tf_bert_for_sequence_classification/classifier/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0', 'tf_bert_for_sequence_classification/classifier/kernel:0', 'tf_bert_for_sequence_classification/classifier/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0', 'tf_bert_for_sequence_classification/classifier/kernel:0', 'tf_bert_for_sequence_classification/classifier/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "471/471 [==============================] - 147s 191ms/step - loss: 0.0688 - accuracy: 0.9407 - precision: 0.0119 - val_loss: 0.0321 - val_accuracy: 0.9661 - val_precision: 0.0114\n",
            "Epoch 2/5\n",
            "471/471 [==============================] - 76s 160ms/step - loss: 0.0318 - accuracy: 0.9719 - precision: 0.0119 - val_loss: 0.0276 - val_accuracy: 0.9769 - val_precision: 0.0114\n",
            "Epoch 3/5\n",
            "471/471 [==============================] - 74s 158ms/step - loss: 0.0210 - accuracy: 0.9838 - precision: 0.0119 - val_loss: 0.0298 - val_accuracy: 0.9779 - val_precision: 0.0114\n",
            "Epoch 4/5\n",
            "471/471 [==============================] - 73s 154ms/step - loss: 0.0155 - accuracy: 0.9886 - precision: 0.0119 - val_loss: 0.0303 - val_accuracy: 0.9777 - val_precision: 0.0114\n",
            "Epoch 5/5\n",
            "471/471 [==============================] - 73s 155ms/step - loss: 0.0115 - accuracy: 0.9917 - precision: 0.0119 - val_loss: 0.0352 - val_accuracy: 0.9793 - val_precision: 0.0114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4UKM3njEfj9"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3bav5DvEfz9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjPC408HZvIr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c5d337a-c20d-4ff5-ef1b-d9390e709081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68/68 [==============================] - 8s 75ms/step\n"
          ]
        }
      ],
      "source": [
        "y_pred = roberta_model.predict(val_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = finbert_model.predict(val_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLYE2O4V3Jb_",
        "outputId": "bcbef693-9c5d-4ad7-81a0-dcb3e986b551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68/68 [==============================] - 13s 89ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7dzr_oMakdq"
      },
      "outputs": [],
      "source": [
        "trials = []\n",
        "for example in y_pred:\n",
        "  trial = []\n",
        "  for row in example:\n",
        "    trial.append(tf.argmax(row, 0, name=None).numpy())\n",
        "\n",
        "  trials.append(trial)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#513, 477, 458"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trials_one = []\n",
        "for example in val_labels.numpy():\n",
        "  trial_one = []\n",
        "  for row in example:\n",
        "    trial_one.append(tf.argmax(row, 0, name=None).numpy())\n",
        "\n",
        "\n",
        "  trials_one.append(trial_one)\n",
        "\n",
        "\n",
        "test = 0\n",
        "count = 0\n"
      ],
      "metadata": {
        "id": "FOToawENvuBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMw8xDs3bxJ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57e3558e-7e55-43ac-dc18-9be96ce32031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "728\n",
            "1421\n"
          ]
        }
      ],
      "source": [
        "test = 0\n",
        "incorrect = 0\n",
        "correct = 0\n",
        "prediction = []\n",
        "trial = []\n",
        "while test < len(val_labels):\n",
        "\n",
        "  if trials_one[test] != trials[test]:\n",
        "    incorrect = incorrect +1\n",
        "    #print(\"Incorrect: \" + str(incorrect))\n",
        "    #print(tokenizer.decode(val_tokens['input_ids'][test]))\n",
        "    #print(\"Incorrect Label:\")\n",
        "    #print(np.argmax(val_labels[test] , axis = 1))\n",
        "    #print(\"Incorrect Prediction:\")\n",
        "    #print(trials[test])\n",
        "    #print(val_decision[test])\n",
        "    prediction.append(0)\n",
        "\n",
        "  if trials_one[test] == trials[test]:\n",
        "    correct = correct +1\n",
        "    prediction.append(1)\n",
        "  test = test + 1\n",
        "print(incorrect)\n",
        "print(correct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm9YSoarbxa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f090349c-42b0-46ee-d0a7-ce283b76ae45"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6612377850162866"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "1421/(1505+644)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVjNxAMbnaKC"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYbXT9jybXJN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-LJG_cR-Ydr"
      },
      "source": [
        "Parsing additional data for Pre-Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5KweBE39EG8"
      },
      "outputs": [],
      "source": [
        "semval = pd.read_csv('/content/gdrive/MyDrive/Raw Data/semval_cleaned.csv')\n",
        "semval_r = semval[[\"title\", \"Decisions\"]]\n",
        "fiqa = pd.read_csv('/content/gdrive/MyDrive/Raw Data/fiqa_cleaned.csv')\n",
        "fiqa_r = fiqa[[\"title\", \"Decisions\"]]\n",
        "\n",
        "\n",
        "\n",
        "pre_train_data = pd.concat([fiqa_r,semval_r],ignore_index = True)\n",
        "\n",
        "\n",
        "pretrain_dataset, pretrain_tokens, pretrain_labels, delete, pretrain_sentence, pretrain_headline, pretrain_decision = clean_data2(pre_train_data['title'],pre_train_data['Decisions'])\n",
        "\n",
        "\n",
        "pre_train_data.drop(delete, axis=0, inplace=True)\n",
        "\n",
        "pretrain_dataset, pretrain_tokens, pretrain_labels, delete, pretrain_sentence, pretrain_headline, pretrain_decision= clean_data2(pre_train_data['title'],pre_train_data['Decisions'])\n",
        "\n",
        "\n",
        "pretrain_labels = to_categorical(pretrain_labels, num_classes=5)\n",
        "pretrain_labels = tf.convert_to_tensor(pretrain_labels )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(semval_r),len(fiqa_r),len(pre_train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9EntTC1momh",
        "outputId": "58331c33-a3be-4a5c-b142-6125ba1c5b2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(966, 436)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMkh9hlg-eEY"
      },
      "source": [
        "##Pre-Train model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMkep3hvNT3C"
      },
      "outputs": [],
      "source": [
        "\n",
        "tf.keras.backend.clear_session()\n",
        "roberta_model_pt = create_roberta_multiclass_model(num_classes=5, hidden_size=120, dropout = 0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-Ic7cDJSkZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8ae53b5-3667-444f-b9bc-139ec5483a5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80/80 [==============================] - 55s 159ms/step - loss: 0.1654 - accuracy: 0.8788 - precision: 0.0114\n",
            "Epoch 2/2\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.0761 - accuracy: 0.9397 - precision: 0.0114\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa164723040>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "\n",
        "roberta_model_pt.fit(pretrain_tokens, pretrain_labels,\n",
        "                                                  batch_size=16,\n",
        "                                                  epochs= 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2Zg2VycjW0I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca688e2-ff2d-4386-cbde-e1fcdf656190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "471/471 [==============================] - 38s 80ms/step - loss: 0.0508 - accuracy: 0.9551 - precision: 0.0119 - val_loss: 0.0290 - val_accuracy: 0.9750 - val_precision: 0.0114\n",
            "Epoch 2/4\n",
            "471/471 [==============================] - 31s 66ms/step - loss: 0.0300 - accuracy: 0.9770 - precision: 0.0119 - val_loss: 0.0271 - val_accuracy: 0.9765 - val_precision: 0.0114\n",
            "Epoch 3/4\n",
            "471/471 [==============================] - 29s 62ms/step - loss: 0.0226 - accuracy: 0.9824 - precision: 0.0119 - val_loss: 0.0263 - val_accuracy: 0.9788 - val_precision: 0.0114\n",
            "Epoch 4/4\n",
            "471/471 [==============================] - 29s 62ms/step - loss: 0.0179 - accuracy: 0.9864 - precision: 0.0119 - val_loss: 0.0299 - val_accuracy: 0.9795 - val_precision: 0.0114\n"
          ]
        }
      ],
      "source": [
        "\n",
        "roberta_model_pt_history = roberta_model_pt.fit(train_tokens, train_labels,\n",
        "\n",
        "                                                  validation_data=(val_tokens, val_labels),\n",
        "                                                  batch_size=16,\n",
        "                                                  epochs= 4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLWjf5FhAEFt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d553b46-fde0-4ec9-a720-714416da6a28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68/68 [==============================] - 5s 25ms/step\n"
          ]
        }
      ],
      "source": [
        "y_pred_pretrain = roberta_model_pt.predict(val_tokens)\n",
        "\n",
        "\n",
        "test = 0\n",
        "count = 0\n",
        "trials_p = []\n",
        "for example in y_pred_pretrain:\n",
        "  trial = []\n",
        "  for row in example:\n",
        "    trial.append(tf.argmax(row, 0, name=None).numpy())\n",
        "\n",
        "  trials_p.append(trial)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U894iDrf7r8B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "a7b68236-3853-4bc4-d65a-cf20d7331ba8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-e0bbdb1173a7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpred_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mtrials_one\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtrials_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mcorrect_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_p\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpred_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trials_one' is not defined"
          ]
        }
      ],
      "source": [
        "test = 0\n",
        "incorrect_p = 0\n",
        "correct_p = 0\n",
        "pred_p = []\n",
        "while test < len(val_labels):\n",
        "  if trials_one[test] == trials_p[test]:\n",
        "    correct_p = correct_p +1\n",
        "    pred_p.append(1)\n",
        "\n",
        "  if trials_one[test] != trials_p[test]:\n",
        "    incorrect_p = incorrect_p +1\n",
        "    pred_p.append(0)\n",
        "  test = test + 1\n",
        "print(correct_p)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre-Train model"
      ],
      "metadata": {
        "id": "M78vRlwo_qyA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZmSo5ZEAl9A"
      },
      "outputs": [],
      "source": [
        "\n",
        "tf.keras.backend.clear_session()\n",
        "roberta_model_pt_2 = create_roberta_multiclass_model( num_classes=5, hidden_size=120, dropout = 0.15)\n",
        "\n",
        "roberta_model_pt_2.fit(pretrain_tokens, pretrain_labels,\n",
        "                                                  batch_size=16,\n",
        "                                                  epochs=2)\n",
        "roberta_model_pt_history_2 = roberta_model_pt_2.fit(train_tokens, train_labels,\n",
        "                                                  validation_data=(val_tokens, val_labels),\n",
        "                                                  batch_size=16,\n",
        "                                                  epochs=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmUZLRZ0t5fP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BjWILF9ZqyH"
      },
      "outputs": [],
      "source": [
        "predictions = pd.read_csv('/content/gdrive/MyDrive/Raw Data/predictions/roberta_pt.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DflooFQaSrG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYU1Vay_OE8E"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgHESixvB6Jf",
        "outputId": "924cca8c-7823-4921-eeb5-0bed8834e0f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typeguard<3.0.0,>=2.7\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow-addons) (23.0)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.20.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2yvQVj3EMx3",
        "outputId": "dcd8db17-41ca-4977-c211-75d98e9c5b8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.layers import CRF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VX7fzAw-n2A"
      },
      "source": [
        "##Implementing CRF for Roberta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fltQmJhs6sgJ"
      },
      "outputs": [],
      "source": [
        "max_length = 30\n",
        "\n",
        "\n",
        "\n",
        "def create_roberta_crf_model(model = model,\n",
        "                                 num_classes = 5,\n",
        "                                 hidden_size = 30,\n",
        "                                 dropout=0.2,\n",
        "                                 learning_rate = 0.00005):\n",
        "\n",
        "    keras.backend.clear_session()\n",
        "    bert_model = model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    bert_model.trainable = True\n",
        "\n",
        "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='input_ids')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='attention_mask')\n",
        "\n",
        "    bert_inputs = {'input_ids': input_ids,\n",
        "                  'attention_mask': attention_mask}\n",
        "\n",
        "    bert_out = bert_model(bert_inputs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    output_layer_1 = bert_out['hidden_states'][11]\n",
        "\n",
        "    hidden_1 = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer_1')(output_layer_1)\n",
        "    hidden_1 = tf.keras.layers.Dropout(dropout)(hidden_1)\n",
        "    decoded_sequence, classification_1, sequence_length, chain_kernel= tfa.layers.CRF(5)(hidden_1)\n",
        "    classification_1 = tf.keras.layers.Dense(5, activation='softmax',name='classification_layer_1')(classification_1)\n",
        "\n",
        "\n",
        "    output_layer_2 = bert_out['hidden_states'][10]\n",
        "    hidden_2 = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer_2')(output_layer_2)\n",
        "    hidden_2 = tf.keras.layers.Dropout(dropout)(hidden_2)\n",
        "    decoded_sequence, classification_2, sequence_length, chain_kernel= tfa.layers.CRF(5)(hidden_2)\n",
        "    classification_2 = tf.keras.layers.Dense(5, activation='softmax',name='classification_layer_2')(classification_2)\n",
        "\n",
        "    print(classification_2)\n",
        "    output_layer_3 = bert_out['hidden_states'][9]\n",
        "    hidden_3 = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer_3')(output_layer_3)\n",
        "    hidden_3 = tf.keras.layers.Dropout(dropout)(hidden_3)\n",
        "    decoded_sequence, classification_3, sequence_length, chain_kernel= tfa.layers.CRF(5)(hidden_3)\n",
        "    classification_3 = tf.keras.layers.Dense(5, activation='softmax',name='classification_layer_3')(classification_3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    output_layer_4 = bert_out['hidden_states'][8]\n",
        "    hidden_4 = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer_4')(output_layer_4)\n",
        "    hidden_4= tf.keras.layers.Dropout(dropout)(hidden_4)\n",
        "    decoded_sequence, classification_4, sequence_length, chain_kernel= tfa.layers.CRF(5)(hidden_4)\n",
        "    classification_4 = tf.keras.layers.Dense(5, activation='softmax',name='classification_layer_4')(classification_4)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    concatted = tf.keras.layers.Average()([classification_1, classification_2, classification_3, classification_4])\n",
        "\n",
        "    classification_model = tf.keras.Model(inputs=[input_ids,attention_mask], outputs=[concatted])\n",
        "\n",
        "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "                                metrics=['accuracy', tf.keras.metrics.Precision(thresholds=0, class_id = 2), ])\n",
        "\n",
        "    return classification_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKq27ukb-2xJ",
        "outputId": "ece0a9d3-f5d7-4ff0-9ebf-21c334ec1402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 30, 5), dtype=tf.float32, name=None), name='classification_layer_2/Softmax:0', description=\"created by layer 'classification_layer_2'\")\n"
          ]
        }
      ],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCvXaySBQr2e",
        "outputId": "701f9b0b-0679-4e53-c372-e9877a0e3c9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 30, 5), dtype=tf.float32, name=None), name='classification_layer_2/Softmax:0', description=\"created by layer 'classification_layer_2'\")\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/encoder/layer_._11/attention/self/query/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/query/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/key/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/key/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/value/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/value/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_roberta_model/roberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0', 'chain_kernel:0', 'chain_kernel:0', 'chain_kernel:0', 'chain_kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/encoder/layer_._11/attention/self/query/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/query/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/key/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/key/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/value/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/value/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_roberta_model/roberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0', 'chain_kernel:0', 'chain_kernel:0', 'chain_kernel:0', 'chain_kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/encoder/layer_._11/attention/self/query/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/query/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/key/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/key/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/value/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/value/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_roberta_model/roberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0', 'chain_kernel:0', 'chain_kernel:0', 'chain_kernel:0', 'chain_kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/encoder/layer_._11/attention/self/query/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/query/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/key/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/key/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/value/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/value/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_roberta_model/roberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0', 'chain_kernel:0', 'chain_kernel:0', 'chain_kernel:0', 'chain_kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "236/236 [==============================] - 77s 132ms/step - loss: 0.0191 - accuracy: 0.9872 - precision: 0.0119 - val_loss: 0.0307 - val_accuracy: 0.9802 - val_precision: 0.0114\n",
            "Epoch 2/5\n",
            "236/236 [==============================] - 16s 70ms/step - loss: 0.0057 - accuracy: 0.9956 - precision: 0.0119 - val_loss: 0.0322 - val_accuracy: 0.9803 - val_precision: 0.0114\n",
            "Epoch 3/5\n",
            "236/236 [==============================] - 16s 70ms/step - loss: 0.0041 - accuracy: 0.9967 - precision: 0.0119 - val_loss: 0.0345 - val_accuracy: 0.9813 - val_precision: 0.0114\n",
            "Epoch 4/5\n",
            "236/236 [==============================] - 16s 67ms/step - loss: 0.0033 - accuracy: 0.9974 - precision: 0.0119 - val_loss: 0.0395 - val_accuracy: 0.9814 - val_precision: 0.0114\n",
            "Epoch 5/5\n",
            "236/236 [==============================] - 16s 67ms/step - loss: 0.0032 - accuracy: 0.9974 - precision: 0.0119 - val_loss: 0.0375 - val_accuracy: 0.9812 - val_precision: 0.0114\n"
          ]
        }
      ],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "\n",
        "roberta_crf = create_roberta_crf_model(num_classes=5, hidden_size=120, dropout = 0.20)\n",
        "\n",
        "\n",
        "roberta_crf_history = roberta_crf.fit(train_tokens, train_labels,\n",
        "                                                  validation_data=(val_tokens, val_labels),\n",
        "                                                  batch_size=32,\n",
        "\n",
        "                                                  epochs=5\n",
        "                                                  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3VynNfzEP7G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs6T-okvLa3T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f932cd8c-3d2b-4175-c7ce-3733ec0eb5f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 30, 5), dtype=tf.float32, name=None), name='classification_layer_2/Softmax:0', description=\"created by layer 'classification_layer_2'\")\n",
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/encoder/layer_._11/attention/self/query/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/query/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/key/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/key/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/value/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/value/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_roberta_model/roberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0', 'chain_kernel:0', 'chain_kernel:0', 'chain_kernel:0', 'chain_kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/encoder/layer_._11/attention/self/query/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/query/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/key/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/key/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/value/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/value/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_roberta_model/roberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0', 'chain_kernel:0', 'chain_kernel:0', 'chain_kernel:0', 'chain_kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/encoder/layer_._11/attention/self/query/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/query/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/key/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/key/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/value/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/value/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_roberta_model/roberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0', 'chain_kernel:0', 'chain_kernel:0', 'chain_kernel:0', 'chain_kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/encoder/layer_._11/attention/self/query/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/query/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/key/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/key/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/value/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/self/value/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_roberta_model/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_roberta_model/roberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/dense/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/dense/bias:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_roberta_model/roberta/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0', 'chain_kernel:0', 'chain_kernel:0', 'chain_kernel:0', 'chain_kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80/80 [==============================] - 71s 193ms/step - loss: 0.0990 - accuracy: 0.9151 - precision: 0.0114\n",
            "Epoch 2/2\n",
            "80/80 [==============================] - 11s 139ms/step - loss: 0.0472 - accuracy: 0.9543 - precision: 0.0114\n",
            "Epoch 1/4\n",
            "236/236 [==============================] - 75s 315ms/step - loss: 0.0147 - accuracy: 0.9883 - precision: 0.0119 - val_loss: 0.0269 - val_accuracy: 0.9795 - val_precision: 0.0114\n",
            "Epoch 2/4\n",
            "236/236 [==============================] - 57s 239ms/step - loss: 0.0097 - accuracy: 0.9917 - precision: 0.0119 - val_loss: 0.0279 - val_accuracy: 0.9808 - val_precision: 0.0114\n",
            "Epoch 3/4\n",
            "236/236 [==============================] - 56s 238ms/step - loss: 0.0080 - accuracy: 0.9932 - precision: 0.0119 - val_loss: 0.0286 - val_accuracy: 0.9812 - val_precision: 0.0114\n",
            "Epoch 4/4\n",
            "236/236 [==============================] - 56s 236ms/step - loss: 0.0065 - accuracy: 0.9945 - precision: 0.0119 - val_loss: 0.0305 - val_accuracy: 0.9805 - val_precision: 0.0114\n"
          ]
        }
      ],
      "source": [
        "\n",
        "roberta_crf_2 = create_roberta_crf_model(num_classes=5, hidden_size=120, dropout = 0.20)\n",
        "\n",
        "roberta_crf_2.fit(pretrain_tokens, pretrain_labels,\n",
        "                                                  batch_size=16,\n",
        "                                                  epochs=2)\n",
        "\n",
        "\n",
        "roberta_crf_2_history = roberta_crf_2.fit(train_tokens, train_labels,\n",
        "                                                  validation_data=(val_tokens, val_labels),\n",
        "                                                  batch_size=32,\n",
        "\n",
        "                                                  epochs=4\n",
        "                                                  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZzKBMlwNH6Y",
        "outputId": "da8e1119-3289-4391-d628-42f37523b177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68/68 [==============================] - 19s 86ms/step\n"
          ]
        }
      ],
      "source": [
        "y_crf_new= roberta_crf_2.predict(val_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReHA2N6vHrP3"
      },
      "outputs": [],
      "source": [
        "trials = []\n",
        "for example in y_crf_new:\n",
        "  trial = []\n",
        "  for row in example:\n",
        "    trial.append(tf.argmax(row, 0, name=None).numpy())\n",
        "\n",
        "  trials.append(trial)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Tgnn67uIEWI",
        "outputId": "896fea06-e273-4ac0-b22f-6d20dc4736e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "664\n",
            "1485\n"
          ]
        }
      ],
      "source": [
        "test = 0\n",
        "incorrect = 0\n",
        "correct = 0\n",
        "prediction = []\n",
        "trial = []\n",
        "while test < len(val_labels):\n",
        "\n",
        "  if trials_one[test] != trials[test]:\n",
        "    incorrect = incorrect +1\n",
        "    #print()\n",
        "    #print(\"Incorrect: \" + str(incorrect))\n",
        "    #print(tokenizer.decode(val_tokens['input_ids'][test]))\n",
        "    #print(\"Incorrect Label:\")\n",
        "    #print(np.argmax(val_labels[test] , axis = 1))\n",
        "    #print(\"Incorrect Prediction:\")\n",
        "    #print(trials[test])\n",
        "    #print(val_decision[test])\n",
        "    prediction.append(0)\n",
        "\n",
        "  if trials_one[test] == trials[test]:\n",
        "    correct = correct +1\n",
        "    #print()\n",
        "    #print(\"Correct: \" + str(correct))\n",
        "    #print(tokenizer.decode(val_tokens['input_ids'][test]))\n",
        "    #print(\"Correct Label:\")\n",
        "    #print(np.argmax(val_labels[test] , axis = 1))\n",
        "    #print(\"Correct Prediction:\")\n",
        "    #print(trials[test])\n",
        "    #print(val_decision[test])\n",
        "    prediction.append(1)\n",
        "  test = test + 1\n",
        "print(incorrect)\n",
        "print(correct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNHXmEM_KFVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85218405-8392-4959-ae57-4713ea6d3968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report: Example for Original Predictions\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.99      0.99      0.99     56916\n",
            "       words       0.92      0.96      0.94      4726\n",
            "    negative       0.80      0.84      0.82       732\n",
            "     nuetral       0.80      0.75      0.77      1098\n",
            "    positive       0.85      0.85      0.85       998\n",
            "\n",
            "    accuracy                           0.98     64470\n",
            "   macro avg       0.87      0.88      0.87     64470\n",
            "weighted avg       0.98      0.98      0.98     64470\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_true= tf.reshape(tf.argmax(val_labels, 2), 2149*30)\n",
        "\n",
        "\n",
        "orig_predictions = tf.reshape(tf.argmax(y_pred, 2), 2149*30)\n",
        "target_names = ['class 0', 'words', 'negative','nuetral','positive']\n",
        "\n",
        "print(\"Classification Report: Example for Original Predictions\")\n",
        "print(classification_report(y_true, orig_predictions, target_names=target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaQ6YGhXDiGV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dfae99f-d37d-4774-e9fc-e58d7083f96c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report: Example for Pred Pre Train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.99      0.99      0.99     56916\n",
            "       words       0.92      0.95      0.94      4726\n",
            "    negative       0.81      0.84      0.82       732\n",
            "     nuetral       0.76      0.76      0.76      1098\n",
            "    positive       0.81      0.85      0.83       998\n",
            "\n",
            "    accuracy                           0.98     64470\n",
            "   macro avg       0.86      0.88      0.87     64470\n",
            "weighted avg       0.98      0.98      0.98     64470\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_true= tf.reshape(tf.argmax(val_labels, 2), 2149*30)\n",
        "\n",
        "\n",
        "pre_train_pred = tf.reshape(tf.argmax(y_pred_pretrain, 2), 2149*30)\n",
        "\n",
        "\n",
        "\n",
        "target_names = ['class 0', 'words', 'negative','nuetral','positive']\n",
        "\n",
        "print(\"Classification Report: Example for Pred Pre Train\")\n",
        "print(classification_report(y_true, pre_train_pred, target_names=target_names))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZbBVap7pfKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beb7865c-c75d-4eb1-c544-3b4fecb330fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report: Example for CRF\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.99      0.99      0.99     56916\n",
            "       words       0.93      0.95      0.94      4726\n",
            "    negative       0.85      0.83      0.84       732\n",
            "     nuetral       0.79      0.80      0.80      1098\n",
            "    positive       0.85      0.86      0.86       998\n",
            "\n",
            "    accuracy                           0.98     64470\n",
            "   macro avg       0.88      0.89      0.89     64470\n",
            "weighted avg       0.98      0.98      0.98     64470\n",
            "\n"
          ]
        }
      ],
      "source": [
        "crf_pred = tf.reshape(tf.argmax(y_crf_new, 2), 2149*30)\n",
        "\n",
        "\n",
        "\n",
        "target_names = ['class 0', 'words', 'negative','nuetral','positive']\n",
        "\n",
        "print(\"Classification Report: Example for CRF\")\n",
        "print(classification_report(y_true, crf_pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-vPiUZN3xXV"
      },
      "outputs": [],
      "source": [
        "crf_pred = tf.reshape(tf.argmax(y_crf_new, 2), 1613*30)\n",
        "\n",
        "y_true= tf.reshape(tf.argmax(val_labels, 2), 1613*30)\n",
        "\n",
        "target_names = ['class 0', 'words', 'negative','nuetral','positive']\n",
        "\n",
        "print(\"Classification Report: Example for CRF + Pre-Train\")\n",
        "print(classification_report(y_true, crf_pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUT6SVVYllo3",
        "outputId": "a32a4dae-1c9c-4102-c555-d737a59148f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68/68 [==============================] - 7s 99ms/step\n",
            "34/34 [==============================] - 4s 107ms/step\n"
          ]
        }
      ],
      "source": [
        "# = tf.argmax(roberta_crf.predict(val_tokens),2).numpy()\n",
        "#crf_test = tf.argmax(roberta_crf.predict(test_tokens),2).numpy()\n",
        "\n",
        "#pt_val = tf.argmax(roberta_model_pt.predict(val_tokens),2).numpy()\n",
        "#pt_test = tf.argmax(roberta_model_pt.predict(test_tokens),2).numpy()\n",
        "\n",
        "#val = tf.argmax(roberta_model.predict(val_tokens),2).numpy()\n",
        "#test = tf.argmax(roberta_model.predict(test_tokens),2).numpy()\n",
        "\n",
        "finbert_v = tf.argmax(finbert_model.predict(val_tokens),2).numpy()\n",
        "finbert_t = tf.argmax(finbert_model.predict(test_tokens),2).numpy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z18X9XVsmzdK",
        "outputId": "bca8f5a4-857f-4a3a-bb14-7b8add058827"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([0, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0])]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "to_csv = []\n",
        "for prediction in finbert_t:\n",
        "  to_csv.append([prediction])\n",
        "\n",
        "to_csv[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gOws6I5Kztd"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "pd.DataFrame(to_csv, columns=['predictions']).to_csv('/content/gdrive/MyDrive/Raw Data/predictions/finbert_test.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1WnNwhYn3wA"
      },
      "outputs": [],
      "source": [
        "to_csv = []\n",
        "for prediction in pt_test:\n",
        "  to_csv.append([prediction])\n",
        "\n",
        "to_csv[6]\n",
        "pd.DataFrame(to_csv, columns=['predictions']).to_csv('/content/gdrive/MyDrive/Raw Data/predictions/base_pt_testF.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMNB61RBn4yx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "240761d5-adc9-422a-b42e-345bab5b272b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 4, 1, ..., 0, 0, 0],\n",
              "       [0, 4, 1, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 3, 1, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "pt_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kei3CYHEoM3y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}