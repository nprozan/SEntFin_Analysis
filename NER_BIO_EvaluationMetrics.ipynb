{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "S8166blyAANn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45f3d86b-00ae-41e3-9d9b-369ab90fa17c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.53.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.6.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.22.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.8)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.4.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers\n",
        "!pip install -U tensorflow\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "\n",
        "import sklearn as sk\n",
        "import os\n",
        "import nltk\n",
        "from nltk.data import find\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "import ast\n",
        "from keras import backend as K\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizerFast, TFRobertaModel, AutoModelForSequenceClassification, AutoTokenizer\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "model = TFRobertaModel.from_pretrained('roberta-base',output_hidden_states=True, output_attentions=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBmoREk8APoZ",
        "outputId": "b3ac372e-7d8d-43e7-b42c-c0af226779b2"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer2 = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "\n",
        "model2 = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")"
      ],
      "metadata": {
        "id": "zD2Q0fJptg6U"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "drive.mount('/content/gdrive')\n",
        "sentfin = pd.read_csv('/content/gdrive/MyDrive/Raw Data/SEntFiN-v1.1.csv')\n",
        "from keras.layers import Input, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = np.arange(0,10753)\n",
        "y = np.arange(0,10753)\n",
        "x_train, x_test, y_train, y_test = train_test_split(sentfin['Title'], sentfin['Decisions'], test_size=0.3,  random_state=8)\n",
        "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.666000, random_state=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMWsMixqAbL4",
        "outputId": "c59609b2-4acc-4851-9945-284457110931"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "szpafkcU5HhB"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Function Helps catch all special cases in the tokenization function\n",
        "\n",
        "def return_helper(ex_title, ex):\n",
        "    try: \n",
        "      return_index = ex_title.index(ex+ \":\") \n",
        "    except: \n",
        "      try: \n",
        "        return_index = ex_title.index(ex+ \",\")\n",
        "      except:\n",
        "        try: \n",
        "          return_index = ex_title.index(ex+ \"'s\")\n",
        "        except:\n",
        "          try: \n",
        "            return_index = ex_title.index(ex+ \";\")\n",
        "          except: \n",
        "            try: \n",
        "              return_index = ex_title.index(ex+ \"?\")\n",
        "            except:\n",
        "              try:\n",
        "                return_index = ex_title.index(ex+ \"'\")\n",
        "              except:\n",
        "                try:\n",
        "                  return_index = ex_title.index(ex+ \"s\")\n",
        "                except:\n",
        "                  try:\n",
        "                    return_index = ex_title.index(ex + \"s'\")\n",
        "                    \n",
        "                  except:\n",
        "                    try:\n",
        "                      return_index = ex_title.index(ex + 's:')\n",
        "                      \n",
        "                    except:\n",
        "                      return None\n",
        "    return return_index\n",
        "\n"
      ],
      "metadata": {
        "id": "gn8q0WJLArwl"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data2(x_data,y_data, tokenizer= tokenizer):\n",
        "  '''input: news headline data set with sentiments\n",
        "  output: roberta tokenized inputs, and corresponding label\n",
        "  '''\n",
        "\n",
        "  title =[str(i) for i in x_data.values]\n",
        "  headlines = []\n",
        "  delete = []\n",
        "\n",
        "  for row in title:\n",
        "    headline = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in row]).split())\n",
        "    headlines.append(headline)\n",
        "  try:\n",
        "    decisions = ([json.loads(i) for i in y_data])\n",
        "  except:\n",
        "    decisions = (([ast.literal_eval(i) for i in y_data]))\n",
        "  entities = []\n",
        "  sents = []\n",
        "  for ex in decisions:\n",
        "    r_entities = []\n",
        "    r_sents = []\n",
        "    for key in ex.keys():\n",
        "      r_entities.append(\" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in key]).split()))\n",
        "    for value in ex.values():\n",
        "      r_sents.append(value)\n",
        "    entities.append(r_entities)\n",
        "    sents.append(r_sents)\n",
        "\n",
        "  title_tokenizer = tokenizer(headlines, padding='max_length', max_length=30, truncation=True, return_tensors=\"tf\")\n",
        "  text_tokenized = tokenizer.batch_encode_plus(headlines, padding='max_length', max_length=30, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "  problems = 0 \n",
        "  i = 0\n",
        "  fin_labels = []\n",
        "  while i < len(y_data):\n",
        "    \n",
        "\n",
        "    ex_title = headlines[i].split()\n",
        "    \n",
        "    \n",
        "    indexes = []\n",
        "\n",
        "    input = np.array(text_tokenized.word_ids(i))\n",
        "\n",
        "    for ex_entity in entities[i]:\n",
        "      word_ent = []\n",
        "      ex = ex_entity.split()[0]\n",
        "      try:\n",
        "        return_index = ex_title.index(ex)\n",
        "      \n",
        "        matches = [j for j,val in enumerate(input) if val==return_index]\n",
        "        \n",
        "        for matching in matches:\n",
        "          word_ent.append(matching) \n",
        "    \n",
        "      except:\n",
        "        helper = return_helper(ex_title, ex)\n",
        "        if helper is None:\n",
        "          problems = problems + 1\n",
        "          \n",
        "          delete.append(i)\n",
        "          continue\n",
        "        return_index = helper\n",
        "        matches = [j for j,val in enumerate(input) if val==return_index]\n",
        "        for matching in matches:\n",
        "          word_ent.append(matching)\n",
        "      if len(ex_entity.split()) > 1:\n",
        "        for ex in ex_entity.split()[1:]:\n",
        "          \n",
        "          return_index = return_index + 1\n",
        "          matches = [j for j,val in enumerate(input) if val==return_index]\n",
        "          for matching in matches:\n",
        "              word_ent.append(matching)\n",
        "\n",
        "\n",
        " \n",
        "      indexes.append(word_ent)\n",
        " \n",
        "    return_label = np.zeros(30)\n",
        "    z = 0\n",
        "    while z < len(indexes):\n",
        "      x = 0 \n",
        "      while x < len(indexes[z]):\n",
        "        if x == 0:\n",
        "          \n",
        "          if sents[i][z] == 'positive':\n",
        "            return_label[indexes[z][x]] = 4\n",
        "          elif sents[i][z] == 'neutral':\n",
        "            return_label[indexes[z][x]] = 3\n",
        "          else: \n",
        "            return_label[indexes[z][x]] = 2\n",
        "        else: \n",
        "          return_label[indexes[z][x]] = 1\n",
        "        x = x + 1\n",
        "      z = z + 1\n",
        "\n",
        "    \n",
        "    \n",
        "    fin_labels.append(return_label)\n",
        "    i = i+1\n",
        "\n",
        "  \n",
        "  labels = tf.convert_to_tensor(fin_labels, dtype=tf.float32)\n",
        "  \n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((\n",
        "      dict(title_tokenizer),\n",
        "      labels\n",
        "  ))\n",
        "\n",
        "  return dataset, dict(title_tokenizer), labels, delete,  title , headlines, decisions"
      ],
      "metadata": {
        "id": "soJDKrvLBV1h"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, train_tokens, trained_labels,train_delete, train_sentence, train_headline, train_decision = clean_data2(x_train,y_train)\n",
        "\n",
        "\n",
        "test_dataset, test_tokens, test_labels, test_delete, test_sentence, test_headline, test_decision= clean_data2(x_test,y_test)\n",
        "val_dataset, val_tokens, valid_labels,val_delete, val_sentence, val_headline, val_decision  = clean_data2(x_val, y_val)\n",
        "\n",
        "from keras.utils.np_utils import to_categorical   \n",
        "\n",
        "train_labels = to_categorical(trained_labels, num_classes=5)\n",
        "train_labels = tf.convert_to_tensor(train_labels )\n",
        "\n",
        "val_labels = to_categorical(valid_labels, num_classes=5)\n",
        "val_labels = tf.convert_to_tensor(val_labels )\n",
        "\n",
        "test_labels = to_categorical(test_labels, num_classes=5)\n",
        "test_labels = tf.convert_to_tensor(test_labels )"
      ],
      "metadata": {
        "id": "DU0ZGvkCAdNp"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_val = pd.read_csv('/content/gdrive/MyDrive/Raw Data/predictions/base_valF.csv')\n",
        "roberta_test = pd.read_csv('/content/gdrive/MyDrive/Raw Data/predictions/base_testF.csv')\n",
        "\n",
        "\n",
        "pt_val = pd.read_csv('/content/gdrive/MyDrive/Raw Data/predictions/base_pt_valF.csv')\n",
        "pt_test = pd.read_csv('/content/gdrive/MyDrive/Raw Data/predictions/base_pt_testF.csv')\n",
        "\n",
        "crf_val = pd.read_csv('/content/gdrive/MyDrive/Raw Data/predictions/crf_valF.csv')\n",
        "crf_test = pd.read_csv('/content/gdrive/MyDrive/Raw Data/predictions/crf_testF.csv')"
      ],
      "metadata": {
        "id": "dJZ_htezBb4_"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_labels_cleaned = []\n",
        "for example in val_labels.numpy():\n",
        "  trial = []\n",
        "  for row in example:\n",
        "    trial.append(tf.argmax(row, 0, name=None).numpy())\n",
        "  val_labels_cleaned.append(trial)\n",
        "\n",
        "\n",
        "test_labels_cleaned = []\n",
        "for example in test_labels.numpy():\n",
        "  trial = []\n",
        "  for row in example:\n",
        "    trial.append(tf.argmax(row, 0, name=None).numpy())\n",
        "  test_labels_cleaned.append(trial)\n"
      ],
      "metadata": {
        "id": "clh-eEtjB496"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels_cleaned_2 = np.array(np.array(test_labels_cleaned).flatten())\n"
      ],
      "metadata": {
        "id": "YX_0ilG9DZAO"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "def clean_predictions(predictions, labels, name = '',label_type = '' ):\n",
        "  print('Results for ' + str(name) + ' on ' + str(label_type))\n",
        "  y_pred = []\n",
        "  for prediction in predictions:\n",
        "    cleaned = prediction.strip('[').strip(']').split()\n",
        "    cleaned = [int(x) for x in cleaned]\n",
        "    y_pred.append(cleaned)\n",
        "\n",
        "\n",
        "\n",
        "  count = 0\n",
        "  incorrect = 0 \n",
        "  correct = 0\n",
        "  correct_pred = []\n",
        "  \n",
        "\n",
        "  while count < len(labels):\n",
        "\n",
        "    if labels[count] != y_pred[count]:\n",
        "      incorrect = incorrect + 1\n",
        "      correct_pred.append(0)\n",
        "      \n",
        "    if labels[count] == y_pred[count]:\n",
        "      correct = correct +1\n",
        "\n",
        "      correct_pred.append(1)\n",
        "    count = count + 1\n",
        "  print('Correct Examples: ' + str(correct))\n",
        "\n",
        "\n",
        "  flatten_pred = np.array(y_pred).flatten()\n",
        "\n",
        "\n",
        "  flatten_labels = np.array(labels).flatten()\n",
        "  \n",
        "\n",
        "  target_names = ['class 0', 'words', 'negative','nuetral','positive']\n",
        "\n",
        "  print(\"Classification Report: Example for \" + str(name) + ' on ' + str(label_type) +' set')\n",
        "  #print(classification_report(flatten_labels, flatten_pred, target_names=target_names))\n",
        "  class_report = classification_report(flatten_labels, flatten_pred, target_names=target_names,digits = 4, output_dict = True)\n",
        "  print()\n",
        "  print()\n",
        "  return correct, class_report, y_pred, correct_pred\n",
        "\n"
      ],
      "metadata": {
        "id": "7tO1tD3YB6eD"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#val_review = clean_predictions(roberta_val['predictions'], val_labels_cleaned, 'Base', 'Validation')\n",
        "#pt_val_review = clean_predictions(pt_val['predictions'], val_labels_cleaned, 'PreTrain', 'Validation')\n",
        "#crf_val_review = clean_predictions(crf_val['predictions'], val_labels_cleaned, 'CRF', 'Validation')\n"
      ],
      "metadata": {
        "id": "FQi8fr0vCGmF"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_review = clean_predictions(roberta_test['predictions'], test_labels_cleaned, 'OLD MODEL: Disregard', 'Test')\n",
        "pt_test_review = clean_predictions(pt_test['predictions'], test_labels_cleaned, 'OLD MODEL', 'Test')\n",
        "crf_test_review = clean_predictions(crf_test['predictions'], test_labels_cleaned, 'OLD MODEL', 'Test')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OHr3ayDCPB6",
        "outputId": "01859323-e035-4b3b-e1a3-327ca0f3e2fd"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for OLD MODEL: Disregard on Test\n",
            "Correct Examples: 728\n",
            "Classification Report: Example for OLD MODEL: Disregard on Test set\n",
            "\n",
            "\n",
            "Results for OLD MODEL on Test\n",
            "Correct Examples: 739\n",
            "Classification Report: Example for OLD MODEL on Test set\n",
            "\n",
            "\n",
            "Results for OLD MODEL on Test\n",
            "Correct Examples: 763\n",
            "Classification Report: Example for OLD MODEL on Test set\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels_cleaned_2 = np.array(np.array(test_labels_cleaned).flatten())\n",
        "test_to_drop = np.array(test_review[2]).flatten()\n",
        "pt_to_drop = np.array(pt_test_review[2]).flatten()\n",
        "crf_to_drop = np.array(crf_test_review[2]).flatten()\n",
        "\n",
        "i = 0\n",
        "to_delete = [] \n",
        "while i < len(test_labels_cleaned_2):\n",
        "  if test_labels_cleaned_2[i] < 2:\n",
        "    to_delete.append(i)\n",
        "  i= i +1\n",
        "to_delete = np.array(to_delete).flatten()\n",
        "len(to_delete)\n",
        "to_delete\n",
        "\n",
        "test_labels_cleaned_3 = np.delete(test_labels_cleaned_2, to_delete)\n",
        "crf_to_drop_2 = np.delete(crf_to_drop, to_delete)\n",
        "pt_to_drop_2 = np.delete(pt_to_drop, to_delete)\n",
        "test_to_drop_2 = np.delete(test_to_drop, to_delete)\n",
        "len(test_labels_cleaned_3), len(test_to_drop_2)\n",
        "\n"
      ],
      "metadata": {
        "id": "uGmWjYFdFWHj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91468298-b17e-47e0-c664-aaa917af7817"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1456, 1456)"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lite_classification(predictions, labels, name = '',label_type = '' ):\n",
        "  target_names = ['class 0', 'words',  'negative','nuetral','positive']\n",
        "\n",
        "\n",
        "  print(\"Classification Report: Example for \" + str(name) + ' on ' + str(label_type) +' set')\n",
        "  print(classification_report(labels,predictions, target_names=target_names))\n",
        "  class_report = classification_report(labels,predictions,  target_names=target_names,digits = 4, output_dict = True,zero_division = \"warn\")\n",
        "  print()\n",
        "  print()\n",
        "  return class_report"
      ],
      "metadata": {
        "id": "0vYmuuiYJfW4"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_review2 = lite_classification(test_to_drop_2, test_labels_cleaned_3, 'Base', 'Test')\n",
        "pt_test_review2 = lite_classification(pt_to_drop_2, test_labels_cleaned_3, 'PreTrain', 'Test')\n",
        "crf_test_review2 = lite_classification(crf_to_drop_2, test_labels_cleaned_3, 'CRF', 'Test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k5s05YOI5Aa",
        "outputId": "68bb7b76-2b3b-4cd4-f5ac-5f3770220bfa"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report: Example for Base on Test set\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.00      0.00      0.00         0\n",
            "       words       0.00      0.00      0.00         0\n",
            "    negative       0.86      0.85      0.85       402\n",
            "     nuetral       0.87      0.74      0.80       580\n",
            "    positive       0.89      0.84      0.86       474\n",
            "\n",
            "    accuracy                           0.80      1456\n",
            "   macro avg       0.52      0.49      0.50      1456\n",
            "weighted avg       0.87      0.80      0.84      1456\n",
            "\n",
            "\n",
            "\n",
            "Classification Report: Example for PreTrain on Test set\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.00      0.00      0.00         0\n",
            "       words       0.00      0.00      0.00         0\n",
            "    negative       0.86      0.85      0.86       402\n",
            "     nuetral       0.90      0.74      0.81       580\n",
            "    positive       0.87      0.88      0.87       474\n",
            "\n",
            "    accuracy                           0.82      1456\n",
            "   macro avg       0.53      0.49      0.51      1456\n",
            "weighted avg       0.88      0.82      0.85      1456\n",
            "\n",
            "\n",
            "\n",
            "Classification Report: Example for CRF on Test set\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.00      0.00      0.00         0\n",
            "       words       0.00      0.00      0.00         0\n",
            "    negative       0.90      0.83      0.86       402\n",
            "     nuetral       0.86      0.79      0.82       580\n",
            "    positive       0.89      0.84      0.86       474\n",
            "\n",
            "    accuracy                           0.82      1456\n",
            "   macro avg       0.53      0.49      0.51      1456\n",
            "weighted avg       0.88      0.82      0.85      1456\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8V3eyHaeKXMk"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sudo_predictions(y_pred, labels):\n",
        "\n",
        "  count = 0\n",
        "  incorrect = 0 \n",
        "  correct = 0\n",
        "  correct_pred = []\n",
        "\n",
        "  while count < len(labels):\n",
        "\n",
        "    \n",
        "    inc = 0\n",
        "    token_correct = True\n",
        "    while inc < 30:\n",
        "\n",
        "      if labels[count][inc] == 0:\n",
        "        if y_pred[count][inc] > 1:\n",
        "          token_correct = False\n",
        "          \n",
        "    \n",
        "      if labels[count][inc] == 1:\n",
        "          if y_pred[count][inc] > 1:\n",
        "            token_correct = False \n",
        "            \n",
        "\n",
        "      if (labels[count][inc] > 1):\n",
        "        if labels[count][inc] != y_pred[count][inc]:\n",
        "          #print(str(labels[count][inc] )+ str(y_pred[count][inc] ))\n",
        "          token_correct = False\n",
        "      inc = inc +1\n",
        "    if token_correct:\n",
        "      correct_pred.append(1)\n",
        "    else:\n",
        "      correct_pred.append(0)\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "    count = count + 1\n",
        "  print(sum(correct_pred))\n",
        "  return correct_pred"
      ],
      "metadata": {
        "id": "6O5JEDkAGSeY"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_adj = sudo_predictions(test_review[2],test_labels_cleaned)\n",
        "pt_adj = sudo_predictions(pt_test_review[2],test_labels_cleaned)\n",
        "crf_adj = sudo_predictions(crf_test_review[2],test_labels_cleaned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbWkzh0kL6AY",
        "outputId": "29116bad-6d61-4356-a5de-9b870998a520"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "780\n",
            "782\n",
            "804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(sudo_predictions(val_review[2],val_labels_cleaned), sudo_predictions(val_review[2],val_labels_cleaned)/2149)\n",
        "#print(sudo_predictions(pt_val_review[2],val_labels_cleaned),sudo_predictions(pt_val_review[2],val_labels_cleaned)/2149) \n",
        "#print(sudo_predictions(crf_val_review[2],val_labels_cleaned), sudo_predictions(crf_val_review[2],val_labels_cleaned)/2149) "
      ],
      "metadata": {
        "id": "tjXYVvF0SbSu"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_f1_score(score):\n",
        "  neg = score['negative']['f1-score']*score['negative']['support']\n",
        "  pos = score['positive']['f1-score']*score['positive']['support']\n",
        "  neu = score['nuetral']['f1-score']*score['nuetral']['support']\n",
        "  support = score['nuetral']['support']+ score['positive']['support'] + score['negative']['support']\n",
        "  return (neg + pos + neu)/support"
      ],
      "metadata": {
        "id": "OC-ZzJgmGxFx"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(weighted_f1_score(crf_test_review[1]))\n",
        "print(weighted_f1_score(pt_test_review[1]))\n",
        "print(weighted_f1_score(test_review[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZK39-RxUxMS",
        "outputId": "592e57a5-e240-4cb4-def0-292252b4ba3f"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8166801613726925\n",
            "0.8140735315472858\n",
            "0.8103816987513027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crf_test_review[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWECZ-TURJlL",
        "outputId": "431c824f-24aa-458e-9e17-7d79dc2a7501"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'class 0': {'precision': 0.9924399592109427,\n",
              "  'recall': 0.9929287598944591,\n",
              "  'f1-score': 0.9926842993809791,\n",
              "  'support': 28425},\n",
              " 'words': {'precision': 0.9445593711212247,\n",
              "  'recall': 0.9398929600658708,\n",
              "  'f1-score': 0.9422203879488237,\n",
              "  'support': 2429},\n",
              " 'negative': {'precision': 0.8413098236775819,\n",
              "  'recall': 0.8308457711442786,\n",
              "  'f1-score': 0.8360450563204005,\n",
              "  'support': 402},\n",
              " 'nuetral': {'precision': 0.7775891341256367,\n",
              "  'recall': 0.7896551724137931,\n",
              "  'f1-score': 0.7835757057313942,\n",
              "  'support': 580},\n",
              " 'positive': {'precision': 0.8461538461538461,\n",
              "  'recall': 0.8354430379746836,\n",
              "  'f1-score': 0.840764331210191,\n",
              "  'support': 474},\n",
              " 'accuracy': 0.9809656453110492,\n",
              " 'macro avg': {'precision': 0.8804104268578465,\n",
              "  'recall': 0.8777531402986171,\n",
              "  'f1-score': 0.8790579561183577,\n",
              "  'support': 32310},\n",
              " 'weighted avg': {'precision': 0.9809571563916026,\n",
              "  'recall': 0.9809656453110492,\n",
              "  'f1-score': 0.9809591719959972,\n",
              "  'support': 32310}}"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = x_test.reset_index()"
      ],
      "metadata": {
        "id": "eSHKRrHbHfcy"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['PT'] = pt_test_review[3]\n",
        "data['CRF'] = crf_test_review[3]\n",
        "data['BASE'] = test_review[3]\n",
        "data['CRF_adj'] = crf_adj"
      ],
      "metadata": {
        "id": "P6UxHjr_IIDS"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['SUM'] = data['BASE']+data['CRF']+data['PT']"
      ],
      "metadata": {
        "id": "Ac3kEoMkSZI5"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_merge = y_test.reset_index()\n",
        "data = data.merge(to_merge, on = 'index')"
      ],
      "metadata": {
        "id": "oUZlGjuLS6Mo"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('/content/gdrive/MyDrive/Raw Data/predictions/analysis2.csv')"
      ],
      "metadata": {
        "id": "uYyb3SsaTinS"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, train_tokens, trained_labels,train_delete, train_sentence, train_headline, train_decision = clean_data2(x_train,y_train,tokenizer2)\n",
        "\n",
        "\n",
        "test_dataset, test_tokens, test_labels, test_delete, test_sentence, test_headline, test_decision= clean_data2(x_test,y_test,tokenizer2)\n",
        "val_dataset, val_tokens, valid_labels,val_delete, val_sentence, val_headline, val_decision  = clean_data2(x_val, y_val,tokenizer2)\n",
        " \n",
        "\n",
        "train_labels = to_categorical(trained_labels, num_classes=5)\n",
        "train_labels = tf.convert_to_tensor(train_labels )\n",
        "\n",
        "val_labels = to_categorical(valid_labels, num_classes=5)\n",
        "val_labels = tf.convert_to_tensor(val_labels )\n",
        "\n",
        "test_labels = to_categorical(test_labels, num_classes=5)\n",
        "test_labels = tf.convert_to_tensor(test_labels )\n"
      ],
      "metadata": {
        "id": "O5hwXv3BVDWK"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finbert_v = pd.read_csv('/content/gdrive/MyDrive/Raw Data/predictions/finbert_val.csv')\n",
        "finbert_t = pd.read_csv('/content/gdrive/MyDrive/Raw Data/predictions/finbert_test.csv')"
      ],
      "metadata": {
        "id": "b3H9S4PLETPY"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_labels_cleaned = []\n",
        "for example in val_labels.numpy():\n",
        "  trial = []\n",
        "  for row in example:\n",
        "    trial.append(tf.argmax(row, 0, name=None).numpy())\n",
        "  val_labels_cleaned.append(trial)\n",
        "\n",
        "\n",
        "test_labels_cleaned = []\n",
        "for example in test_labels.numpy():\n",
        "  trial = []\n",
        "  for row in example:\n",
        "    trial.append(tf.argmax(row, 0, name=None).numpy())\n",
        "  test_labels_cleaned.append(trial)"
      ],
      "metadata": {
        "id": "FxbrsAv1EYN8"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finbert_val_review = clean_predictions(finbert_v['predictions'], val_labels_cleaned, 'Finbert', 'Validation')\n",
        "finbert_test_review = clean_predictions(finbert_t['predictions'], test_labels_cleaned, 'Fibert', 'Test')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kA2qLQk0GTBB",
        "outputId": "263dc2a8-2dc6-47b7-afd7-2c3e786713ed"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for Finbert on Validation\n",
            "Correct Examples: 1421\n",
            "Classification Report: Example for Finbert on Validation set\n",
            "\n",
            "\n",
            "Results for Fibert on Test\n",
            "Correct Examples: 702\n",
            "Classification Report: Example for Fibert on Test set\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels_cleaned_2 = np.array(np.array(test_labels_cleaned).flatten())\n",
        "fintest_to_drop = np.array(finbert_test_review[2]).flatten()\n",
        "\n",
        "i = 0\n",
        "to_delete = [] \n",
        "while i < len(test_labels_cleaned_2):\n",
        "  if test_labels_cleaned_2[i] < 2:\n",
        "    to_delete.append(i)\n",
        "  i= i +1\n",
        "to_delete = np.array(to_delete).flatten()\n",
        "len(to_delete)\n",
        "to_delete\n",
        "\n",
        "test_labels_cleaned_3 = np.delete(test_labels_cleaned_2, to_delete)\n",
        "fintest_to_drop_2 = np.delete(fintest_to_drop, to_delete)\n",
        "lite_classification(fintest_to_drop_2, test_labels_cleaned_3, 'Base', 'Test')\n"
      ],
      "metadata": {
        "id": "FdUIYjdMGx7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "247ae9d6-2d0f-4ef2-aa10-93813a621485"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report: Example for Base on Test set\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.00      0.00      0.00         0\n",
            "       words       0.00      0.00      0.00         0\n",
            "    negative       0.90      0.79      0.84       402\n",
            "     nuetral       0.83      0.75      0.79       580\n",
            "    positive       0.83      0.81      0.82       474\n",
            "\n",
            "    accuracy                           0.78      1456\n",
            "   macro avg       0.51      0.47      0.49      1456\n",
            "weighted avg       0.85      0.78      0.81      1456\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'class 0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n",
              " 'words': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n",
              " 'negative': {'precision': 0.8980169971671388,\n",
              "  'recall': 0.7885572139303483,\n",
              "  'f1-score': 0.8397350993377484,\n",
              "  'support': 402},\n",
              " 'nuetral': {'precision': 0.8336520076481836,\n",
              "  'recall': 0.7517241379310344,\n",
              "  'f1-score': 0.7905711695376246,\n",
              "  'support': 580},\n",
              " 'positive': {'precision': 0.8311688311688312,\n",
              "  'recall': 0.810126582278481,\n",
              "  'f1-score': 0.8205128205128205,\n",
              "  'support': 474},\n",
              " 'accuracy': 0.7809065934065934,\n",
              " 'macro avg': {'precision': 0.5125675671968308,\n",
              "  'recall': 0.47008158682797274,\n",
              "  'f1-score': 0.49016381787763874,\n",
              "  'support': 1456},\n",
              " 'weighted avg': {'precision': 0.850614713785139,\n",
              "  'recall': 0.7809065934065934,\n",
              "  'f1-score': 0.8138927645526607,\n",
              "  'support': 1456}}"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xyGf6jTtOtyM"
      },
      "execution_count": 137,
      "outputs": []
    }
  ]
}